\appendix

\section{Mathematical Derivations}
\label{app:derivations}

This appendix provides detailed mathematical derivations omitted from the main text for brevity.

\subsection{Proof of Theorem~\ref{thm:convergence}}
\label{app:convergence_proof}

\begin{proof}
We prove convergence by establishing that the sequence $\{\theta_t\}$ generated by our algorithm satisfies the conditions of [relevant convergence theorem].

\textbf{Step 1: Bounded Sequence}
First, we show that $\{\theta_t\}$ is bounded. From the update rule:
\begin{equation}
\theta_{t+1} = \theta_t - \alpha_t \nabla \mathcal{L}(\theta_t)
\end{equation}

Under Assumption A1 (Lipschitz gradient), we have:
\begin{equation}
\|\nabla \mathcal{L}(\theta)\| \leq L \|\theta\| + C
\end{equation}
for some constants $L, C > 0$.

\textbf{Step 2: Descent Property}
Next, we establish the descent property. For the loss function:
\begin{align}
\mathcal{L}(\theta_{t+1}) &= \mathcal{L}(\theta_t - \alpha_t \nabla \mathcal{L}(\theta_t)) \\
&\leq \mathcal{L}(\theta_t) - \alpha_t \|\nabla \mathcal{L}(\theta_t)\|^2 + \frac{L\alpha_t^2}{2}\|\nabla \mathcal{L}(\theta_t)\|^2
\end{align}

Choosing $\alpha_t < \frac{2}{L}$ ensures:
\begin{equation}
\mathcal{L}(\theta_{t+1}) \leq \mathcal{L}(\theta_t) - \frac{\alpha_t}{2}\|\nabla \mathcal{L}(\theta_t)\|^2
\end{equation}

\textbf{Step 3: Convergence}
Since $\mathcal{L}$ is bounded below and the sequence $\{\mathcal{L}(\theta_t)\}$ is decreasing, it converges. This implies:
\begin{equation}
\sum_{t=0}^{\infty} \|\nabla \mathcal{L}(\theta_t)\|^2 < \infty
\end{equation}

Therefore, $\|\nabla \mathcal{L}(\theta_t)\| \rightarrow 0$ as $t \rightarrow \infty$, establishing convergence to a stationary point.
\end{proof}

\subsection{Derivation of Update Rule}
\label{app:update_derivation}

Consider the optimization problem:
\begin{equation}
\min_{\theta} \mathcal{L}(\theta) + \lambda \mathcal{R}(\theta)
\end{equation}
where $\mathcal{R}(\theta)$ is a regularization term.

Taking the gradient and setting it to zero:
\begin{equation}
\nabla \mathcal{L}(\theta) + \lambda \nabla \mathcal{R}(\theta) = 0
\end{equation}

For $\mathcal{R}(\theta) = \frac{1}{2}\|\theta\|^2$, this gives:
\begin{equation}
\nabla \mathcal{L}(\theta) + \lambda \theta = 0
\end{equation}

The corresponding gradient descent update becomes:
\begin{equation}
\theta_{t+1} = \theta_t - \alpha(\nabla \mathcal{L}(\theta_t) + \lambda \theta_t) = (1-\alpha\lambda)\theta_t - \alpha\nabla \mathcal{L}(\theta_t)
\end{equation}

\section{Experimental Details}
\label{app:experimental}

\subsection{Dataset Descriptions}
\label{app:datasets}

\subsubsection{Dataset 1: [Name]}
\begin{itemize}
    \item \textbf{Source:} [Data source and reference]
    \item \textbf{Size:} [Number of samples, features, etc.]
    \item \textbf{Characteristics:} [Key properties of the dataset]
    \item \textbf{Preprocessing:} [Preprocessing steps applied]
    \item \textbf{Train/Validation/Test Split:} [Split ratios and methodology]
\end{itemize}

\subsubsection{Dataset 2: [Name]}
\begin{itemize}
    \item \textbf{Source:} [Data source and reference]
    \item \textbf{Size:} [Number of samples, features, etc.]
    \item \textbf{Characteristics:} [Key properties of the dataset]
    \item \textbf{Preprocessing:} [Preprocessing steps applied]
    \item \textbf{Train/Validation/Test Split:} [Split ratios and methodology]
\end{itemize}

\subsection{Implementation Details}
\label{app:implementation}

\subsubsection{Software Environment}
\begin{itemize}
    \item \textbf{Programming Language:} [Language and version]
    \item \textbf{Key Libraries:} [List of important libraries and versions]
    \item \textbf{Hardware:} [Hardware specifications used for experiments]
    \item \textbf{Operating System:} [OS details]
\end{itemize}

\subsubsection{Hyperparameter Settings}
\begin{table}[htbp]
\centering
\caption{Hyperparameter settings for all experiments.}
\label{tab:hyperparameters}
\begin{tabular}{lc}
\toprule
Parameter & Value \\
\midrule
Learning Rate ($\alpha$) & $10^{-3}$ \\
Regularization ($\lambda$) & $10^{-4}$ \\
Batch Size & 32 \\
Maximum Iterations & 1000 \\
Convergence Tolerance & $10^{-6}$ \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Computational Complexity Analysis}

The computational complexity of our algorithm is analyzed as follows:

\begin{itemize}
    \item \textbf{Time Complexity per Iteration:} $O(n \log n)$ where $n$ is the problem size
    \item \textbf{Space Complexity:} $O(n)$ for storing intermediate results
    \item \textbf{Total Time Complexity:} $O(T \cdot n \log n)$ where $T$ is the number of iterations
\end{itemize}

The space complexity breakdown:
\begin{align}
\text{Parameter storage:} &\quad O(d) \\
\text{Gradient computation:} &\quad O(n) \\
\text{Intermediate results:} &\quad O(n) \\
\text{Total:} &\quad O(n + d)
\end{align}
where $d$ is the parameter dimension.

\section{Additional Results}
\label{app:additional_results}

\subsection{Extended Experimental Results}
\label{app:extended_results}

This section provides additional experimental results that supplement the main findings.

\subsubsection{Per-Class Performance Analysis}

\begin{table}[htbp]
\centering
\caption{Per-class performance breakdown for Dataset 1.}
\label{tab:per_class}
\begin{tabular}{lcccc}
\toprule
Class & Precision & Recall & F1-Score & Support \\
\midrule
Class 1 & 0.92 & 0.89 & 0.90 & 150 \\
Class 2 & 0.94 & 0.96 & 0.95 & 200 \\
Class 3 & 0.88 & 0.85 & 0.86 & 100 \\
Class 4 & 0.91 & 0.93 & 0.92 & 175 \\
\midrule
Macro Avg & 0.91 & 0.91 & 0.91 & 625 \\
Weighted Avg & 0.92 & 0.92 & 0.92 & 625 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Convergence Analysis}

\begin{figure}[htbp]
\centering
% Placeholder for convergence plots
\fbox{\parbox{0.8\textwidth}{\centering [Figure: Convergence Analysis]\\ Detailed convergence behavior across different datasets and parameter settings}}
\caption{Convergence analysis showing (a) loss vs. iterations, (b) gradient norm vs. iterations, and (c) parameter trajectory for representative runs.}
\label{fig:convergence_analysis}
\end{figure}

\subsection{Robustness Analysis}
\label{app:robustness}

\subsubsection{Noise Sensitivity}

We evaluate robustness to input noise by adding Gaussian noise with varying standard deviations:

\begin{table}[htbp]
\centering
\caption{Performance under different noise levels.}
\label{tab:noise_robustness}
\begin{tabular}{lccc}
\toprule
Noise Level ($\sigma$) & Our Method & Baseline 1 & Baseline 2 \\
\midrule
0.0 & 0.93 & 0.85 & 0.89 \\
0.1 & 0.91 & 0.81 & 0.86 \\
0.2 & 0.88 & 0.76 & 0.82 \\
0.3 & 0.84 & 0.70 & 0.77 \\
0.5 & 0.78 & 0.62 & 0.69 \\
\bottomrule
\end{tabular}
\end{table}

\section{Assumptions and Limitations}
\label{app:assumptions}

\subsection{Mathematical Assumptions}
\label{app:math_assumptions}

Our theoretical analysis relies on the following assumptions:

\begin{assumption}[Lipschitz Continuity]
\label{ass:lipschitz}
The objective function $\mathcal{L}(\theta)$ has Lipschitz continuous gradients with constant $L > 0$:
\begin{equation}
\|\nabla \mathcal{L}(\theta_1) - \nabla \mathcal{L}(\theta_2)\| \leq L \|\theta_1 - \theta_2\|
\end{equation}
\end{assumption}

\begin{assumption}[Bounded Gradients]
\label{ass:bounded_gradients}
The gradients are uniformly bounded:
\begin{equation}
\|\nabla \mathcal{L}(\theta)\| \leq M
\end{equation}
for some constant $M > 0$ and all $\theta$ in the feasible region.
\end{assumption}

\begin{assumption}[Strong Convexity]
\label{ass:strong_convexity}
The objective function is $\mu$-strongly convex:
\begin{equation}
\mathcal{L}(\theta_2) \geq \mathcal{L}(\theta_1) + \nabla \mathcal{L}(\theta_1)^T(\theta_2 - \theta_1) + \frac{\mu}{2}\|\theta_2 - \theta_1\|^2
\end{equation}
\end{assumption}

\subsection{Practical Limitations}
\label{app:practical_limitations}

\begin{enumerate}
    \item \textbf{Computational Scalability:} While our method scales well theoretically, practical implementation may face memory constraints for problems with $n > 10^6$ samples.
    
    \item \textbf{Parameter Sensitivity:} Although robust across reasonable parameter ranges, performance may degrade with extreme parameter choices.
    
    \item \textbf{Data Requirements:} The method requires minimum $n > 100$ samples for stable performance, with optimal results achieved with $n > 1000$.
\end{enumerate}
